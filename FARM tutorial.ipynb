{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by adjust the working directory so that it is the root of the repository\n",
    "# This should be run just once.\n",
    "\n",
    "import os\n",
    "os.chdir('../')\n",
    "print(\"Current working directory is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:07:18.910329 13716 file_utils.py:38] PyTorch version 1.4.0 available.\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "\n",
    "import torch\n",
    "from farm.modeling.tokenization import Tokenizer\n",
    "from farm.data_handler.processor import TextClassificationProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import LanguageModel\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.modeling.optimization import initialize_optimizer\n",
    "from farm.train import Trainer\n",
    "from farm.utils import MLFlowLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " __          __  _                            _        \n",
      " \\ \\        / / | |                          | |       \n",
      "  \\ \\  /\\  / /__| | ___ ___  _ __ ___   ___  | |_ ___  \n",
      "   \\ \\/  \\/ / _ \\ |/ __/ _ \\| '_ ` _ \\ / _ \\ | __/ _ \\ \n",
      "    \\  /\\  /  __/ | (_| (_) | | | | | |  __/ | || (_) |\n",
      "     \\/  \\/ \\___|_|\\___\\___/|_| |_| |_|\\___|  \\__\\___/ \n",
      "  ______      _____  __  __  \n",
      " |  ____/\\   |  __ \\|  \\/  |              _.-^-._    .--.\n",
      " | |__ /  \\  | |__) | \\  / |           .-'   _   '-. |__|\n",
      " |  __/ /\\ \\ |  _  /| |\\/| |          /     |_|     \\|  |\n",
      " | | / ____ \\| | \\ \\| |  | |         /               \\  |\n",
      " |_|/_/    \\_\\_|  \\_\\_|  |_|        /|     _____     |\\ |\n",
      "                                     |    |==|==|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |--|--|    |  |\n",
      "|---||---|---|---|---|---|---|---|---|    |==|==|    |  |\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020/03/20 14:07:45 WARNING mlflow.tracking.context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Farm allows simple logging of many parameters & metrics. Let's use MLflow framework to track our experiment ...\n",
    "ml_logger = MLFlowLogger(tracking_uri=\"https://public-mlflow.deepset.ai/\")\n",
    "ml_logger.init_experiment(experiment_name=\"Public_FARM\", run_name=\"Tutorial1_Colab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices available: cpu\n"
     ]
    }
   ],
   "source": [
    "# We need to fetch the right device to drive the growth of our model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:10:05.839990 13716 tokenization.py:72] Loading tokenizer of type 'BertTokenizer'\n",
      "I0320 14:10:06.002772 13716 tokenization_utils.py:418] loading file https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt from cache at C:\\Users\\ajankowski\\.cache\\torch\\transformers\\da299cdd121a3d71e1626f2908dda0d02658f42e925a3d6abd8273ec08cf41a6.2a48e6c60dcdb582effb718237ce5894652e3b4abb94f0a4d9a857b70333308d\n"
     ]
    }
   ],
   "source": [
    "# Here we initialize a tokenizer that will be used for preprocessing text\n",
    "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
    "# It is currently loaded with a German model\n",
    "\n",
    "tokenizer = Tokenizer.load(\n",
    "    pretrained_model_name_or_path=\"bert-base-german-cased\",\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to prepare the data for the model, we need a set of\n",
    "# functions to transform data files into PyTorch Datasets.\n",
    "# We group these together in Processor objects.\n",
    "# We will need a new Processor object for each new source of data.\n",
    "# The abstract class can be found in farm.data_handling.processor.Processor\n",
    "\n",
    "processor = TextClassificationProcessor(tokenizer=tokenizer,\n",
    "                                        max_seq_len=128,\n",
    "                                        data_dir=\"data/germeval18\",\n",
    "                                        label_list = [\"OTHER\", \"OFFENSE\"],\n",
    "                                        metric = \"f1_macro\",\n",
    "                                        label_column_name = \"coarse_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:26:10.223584 13716 data_silo.py:179] \n",
      "Loading data into the data silo ... \n",
      "              ______\n",
      "               |o  |   !\n",
      "   __          |:`_|---'-.\n",
      "  |__|______.-/ _ \\-----.|       \n",
      " (o)(o)------'\\ _ /     ( )      \n",
      " \n",
      "I0320 14:26:10.225584 13716 data_silo.py:188] Loading train set from: data\\germeval18\\train.tsv \n",
      "I0320 14:26:10.226576 13716 utils.py:39]  Couldn't find data\\germeval18\\train.tsv locally. Trying to download ...\n",
      "I0320 14:26:10.227574 13716 utils.py:145] downloading and extracting file germeval18 to dir C:\\Personal\\AI\\analiza sentymentu\\data\n",
      "100%|██████████████████████████████████████████████████████████████████████| 525101/525101 [00:00<00:00, 3681330.41B/s]\n",
      "I0320 14:26:10.763776 13716 data_silo.py:139] Got ya 7 parallel workers to convert 5009 dictionaries to pytorch datasets (chunksize = 144)...\n",
      "I0320 14:26:10.776438 13716 utils.py:244]  0    0    0    0    0    0    0 \n",
      "I0320 14:26:10.777435 13716 utils.py:244] /w\\  /w\\  /w\\  /w\\  /w\\  /|\\  /w\\\n",
      "I0320 14:26:10.779429 13716 utils.py:244] /'\\  / \\  / \\  / \\  /'\\  /'\\  / \\\n",
      "I0320 14:26:10.780428 13716 utils.py:244]             \n",
      "Preprocessing Dataset: 5040 Dicts [00:09, 536.64 Dicts/s]                                                              \n",
      "I0320 14:26:20.432447 13716 data_silo.py:203] Loading dev set as a slice of train set\n",
      "I0320 14:26:20.453281 13716 data_silo.py:348] Took 545 samples out of train set to create dev set (dev split is roughly 0.1)\n",
      "I0320 14:26:20.455277 13716 data_silo.py:217] Loading test set from: data\\germeval18\\test.tsv\n",
      "I0320 14:26:20.580793 13716 data_silo.py:139] Got ya 7 parallel workers to convert 3532 dictionaries to pytorch datasets (chunksize = 101)...\n",
      "I0320 14:26:20.580793 13716 utils.py:244]  0    0    0    0    0    0    0 \n",
      "I0320 14:26:20.580793 13716 utils.py:244] /w\\  /w\\  /w\\  /w\\  /|\\  /w\\  /|\\\n",
      "I0320 14:26:20.594542 13716 utils.py:244] / \\  / \\  / \\  / \\  /'\\  / \\  /'\\\n",
      "I0320 14:26:20.597533 13716 utils.py:244]             \n",
      "Preprocessing Dataset: 3535 Dicts [00:08, 415.14 Dicts/s]                                                              \n",
      "I0320 14:26:29.264268 13716 data_silo.py:405] Examples in train: 4464\n",
      "I0320 14:26:29.264268 13716 data_silo.py:406] Examples in dev  : 545\n",
      "I0320 14:26:29.264268 13716 data_silo.py:407] Examples in test : 3532\n",
      "I0320 14:26:29.264268 13716 data_silo.py:408] \n",
      "I0320 14:26:29.276966 13716 data_silo.py:409] Max sequence length:     128\n",
      "I0320 14:26:29.278955 13716 data_silo.py:410] Average sequence length after clipping: 40.128136200716845\n",
      "I0320 14:26:29.279956 13716 data_silo.py:411] Proportion clipped:      0.01724910394265233\n"
     ]
    }
   ],
   "source": [
    "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
    "# The DataSilo will call the functions in the Processor to generate these sets.\n",
    "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
    "# be passed on to the model.\n",
    "# Here is a good place to define a batch size for the model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling¶\n",
    "In FARM, we make a strong distinction between the language model and prediction head so that you can mix and match different building blocks for your needs.\n",
    "\n",
    "For example, in the transfer learning paradigm, you might have the one language model that you will be using for both document classification and NER. Or you perhaps you have a pretrained language model which you would like to adapt to your domain, then use for a downstream task such as question answering.\n",
    "\n",
    "All this is possible within FARM and requires only the replacement of a few modular components, as we shall see below.\n",
    "\n",
    "Let's first have a look at how we might set up a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:39:33.548645 13716 filelock.py:274] Lock 2792498684816 acquired on C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e653e2fe0970d519c5a3b6c0286e1630ad2f0eade78f82b4916ec945d6f06d48.4154b6bb468532f5a3035a2e706fc9db941628923ea897f73c727d9c8a9c0d1a.lock\n",
      "I0320 14:39:33.548645 13716 file_utils.py:413] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json not found in cache or force_download set to True, downloading to C:\\Users\\ajankowski\\.cache\\torch\\transformers\\tmpobzprxhe\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a81b0862c24e2bb0db796a7cc44082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=336, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:39:34.170804 13716 file_utils.py:423] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json in cache at C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e653e2fe0970d519c5a3b6c0286e1630ad2f0eade78f82b4916ec945d6f06d48.4154b6bb468532f5a3035a2e706fc9db941628923ea897f73c727d9c8a9c0d1a\n",
      "I0320 14:39:34.171834 13716 file_utils.py:426] creating metadata file for C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e653e2fe0970d519c5a3b6c0286e1630ad2f0eade78f82b4916ec945d6f06d48.4154b6bb468532f5a3035a2e706fc9db941628923ea897f73c727d9c8a9c0d1a\n",
      "I0320 14:39:34.174793 13716 filelock.py:318] Lock 2792498684816 released on C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e653e2fe0970d519c5a3b6c0286e1630ad2f0eade78f82b4916ec945d6f06d48.4154b6bb468532f5a3035a2e706fc9db941628923ea897f73c727d9c8a9c0d1a.lock\n",
      "I0320 14:39:34.772385 13716 filelock.py:274] Lock 2792498684816 acquired on C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e.lock\n",
      "I0320 14:39:34.772385 13716 file_utils.py:413] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\ajankowski\\.cache\\torch\\transformers\\tmp2a7jeaw3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9328b2a716ac45ddbfd8d63896757627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=438869143, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:40:36.326134 13716 file_utils.py:423] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin in cache at C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e\n",
      "I0320 14:40:36.329135 13716 file_utils.py:426] creating metadata file for C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e\n",
      "I0320 14:40:36.333117 13716 filelock.py:318] Lock 2792498684816 released on C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e.lock\n",
      "I0320 14:40:36.334113 13716 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin from cache at C:\\Users\\ajankowski\\.cache\\torch\\transformers\\e32f648561b03f77a129832928b7f16decdc5e0870f1e6558857e046169d4133.4e5eda3a0f09b32a0b7d1a9185034da1b3506d5c5b0c6880a7ca0122ab5eef2e\n",
      "I0320 14:40:38.206928 13716 language_model.py:218] Automatically detected language from language model name: german\n"
     ]
    }
   ],
   "source": [
    "# The language model is the foundation on which modern NLP systems are built.\n",
    "# They encapsulate a general understanding of sentence semantics\n",
    "# and are not specific to any one task.\n",
    "\n",
    "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
    "# The model being loaded is a German model that we trained. \n",
    "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
    "# have saved or download one connected to the HuggingFace repository.\n",
    "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
    "# available models\n",
    "\n",
    "MODEL_NAME_OR_PATH = \"bert-base-german-cased\"\n",
    "\n",
    "language_model = LanguageModel.load(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0320 14:41:46.517942 13716 prediction_head.py:265] `layer_dims` will be deprecated in future releases\n",
      "I0320 14:41:46.519937 13716 prediction_head.py:272] Prediction head initialized with size [768, 2]\n"
     ]
    }
   ],
   "source": [
    "# A prediction head is a model that processes the output of the language model\n",
    "# for a specific task.\n",
    "# Prediction heads will look different depending on whether you're doing text classification\n",
    "# Named Entity Recognition (NER), question answering or some other task.\n",
    "# They should generate logits over the available prediction classes and contain methods\n",
    "# to convert these logits to losses or predictions \n",
    "\n",
    "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
    "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
    "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
    "\n",
    "# Here by default we have a single layer network.\n",
    "# It takes in a vector of length 768 (the default size of BERT's output).\n",
    "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
    "\n",
    "LAYER_DIMS = [768, 2]\n",
    "\n",
    "prediction_head = TextClassificationHead(layer_dims=LAYER_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The language model and prediction head are coupled together in the Adaptive Model.\n",
    "# This class takes care of model saving and loading and also coordinates\n",
    "# cases where there is more than one prediction head.\n",
    "\n",
    "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
    "# language model will be set to zero.\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:44:26.583294 13716 optimization.py:171] Loading optimizer `TransformersAdamW`: '{'correct_bias': False, 'weight_decay': 0.01, 'lr': 2e-05}'\n",
      "I0320 14:44:26.906943 13716 optimization.py:241] Using scheduler 'get_linear_schedule_with_warmup'\n",
      "I0320 14:44:26.906943 13716 optimization.py:255] Loading schedule `get_linear_schedule_with_warmup`: '{'num_warmup_steps': 14.0, 'num_training_steps': 140}'\n"
     ]
    }
   ],
   "source": [
    "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
    "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "N_EPOCHS = 1\n",
    "\n",
    "model, optimizer, lr_schedule = initialize_optimizer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_batches=len(data_silo.loaders[\"train\"]),\n",
    "    n_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop handled by this\n",
    "# It will also trigger evaluation during training using the dev data\n",
    "# and after training using the test data.\n",
    "\n",
    "# Set N_GPU to a positive value if CUDA is available\n",
    "N_GPU = 0\n",
    "\n",
    "trainer = Trainer(model,\n",
    "    optimizer=optimizer,\n",
    "    data_silo=data_silo,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    lr_schedule=lr_schedule,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 14:48:12.871213 13716 train.py:406] \n",
      " \n",
      "\n",
      "          &&& &&  & &&             _____                   _             \n",
      "      && &\\/&\\|& ()|/ @, &&       / ____|                 (_)            \n",
      "      &\\/(/&/&||/& /_/)_&/_&     | |  __ _ __ _____      ___ _ __   __ _ \n",
      "   &() &\\/&|()|/&\\/ '%\" & ()     | | |_ | '__/ _ \\ \\ /\\ / / | '_ \\ / _` |\n",
      "  &_\\_&&_\\ |& |&&/&__%_/_& &&    | |__| | | | (_) \\ V  V /| | | | | (_| |\n",
      "&&   && & &| &| /& & % ()& /&&    \\_____|_|  \\___/ \\_/\\_/ |_|_| |_|\\__, |\n",
      " ()&_---()&\\&\\|&&-&&--%---()~                                       __/ |\n",
      "     &&     \\|||                                                   |___/\n",
      "             |||\n",
      "             |||\n",
      "             |||\n",
      "       , -=-~  .-^- _\n",
      "              `\n",
      "\n",
      "Train epoch 1/1 (Cur. train loss: 0.7012):  72%|██████████████████████████▋          | 101/140 [32:16<12:35, 19.36s/it]\n",
      "Evaluating:   0%|                                                                               | 0/18 [00:00<?, ?it/s]\n",
      "Evaluating:  11%|███████▉                                                               | 2/18 [00:13<01:49,  6.87s/it]\n",
      "Evaluating:  11%|███████▉                                                               | 2/18 [00:26<01:49,  6.87s/it]\n",
      "Evaluating:  22%|███████████████▊                                                       | 4/18 [00:26<01:35,  6.79s/it]\n",
      "Evaluating:  33%|███████████████████████▋                                               | 6/18 [00:39<01:20,  6.69s/it]\n",
      "Evaluating:  44%|███████████████████████████████▌                                       | 8/18 [00:52<01:06,  6.65s/it]\n",
      "Evaluating:  44%|███████████████████████████████▌                                       | 8/18 [01:06<01:06,  6.65s/it]\n",
      "Evaluating:  56%|██████████████████████████████████████▉                               | 10/18 [01:06<00:53,  6.68s/it]\n",
      "Evaluating:  67%|██████████████████████████████████████████████▋                       | 12/18 [01:20<00:40,  6.75s/it]\n",
      "Evaluating:  78%|██████████████████████████████████████████████████████▍               | 14/18 [01:33<00:27,  6.76s/it]\n",
      "Evaluating:  78%|██████████████████████████████████████████████████████▍               | 14/18 [01:46<00:27,  6.76s/it]\n",
      "Evaluating:  89%|██████████████████████████████████████████████████████████████▏       | 16/18 [01:47<00:13,  6.75s/it]\n",
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████| 18/18 [01:53<00:00,  6.33s/it]I0320 15:22:42.195943 13716 eval.py:162] \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | DEV SET | AFTER 100 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "I0320 15:22:42.198936 13716 eval.py:165] \n",
      " _________ text_classification _________\n",
      "I0320 15:22:42.373797 13716 eval.py:185] loss: 0.4046901049417093\n",
      "I0320 15:22:42.373797 13716 eval.py:185] task_name: text_classification\n",
      "I0320 15:22:42.542026 13716 eval.py:185] f1_macro: 0.7811583011583012\n",
      "I0320 15:22:42.557389 13716 eval.py:182] report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       OTHER     0.8480    0.8712    0.8595       365\n",
      "     OFFENSE     0.7235    0.6833    0.7029       180\n",
      "\n",
      "    accuracy                         0.8092       545\n",
      "   macro avg     0.7858    0.7773    0.7812       545\n",
      "weighted avg     0.8069    0.8092    0.8077       545\n",
      "\n",
      "Train epoch 1/1 (Cur. train loss: 0.2321): 100%|█████████████████████████████████████| 140/140 [45:52<00:00, 16.21s/it]\n",
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 111/111 [12:12<00:00,  6.60s/it]\n",
      "I0320 15:46:18.135962 13716 eval.py:162] \n",
      "\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "***************************************************\n",
      "***** EVALUATION | TEST SET | AFTER 139 BATCHES *****\n",
      "***************************************************\n",
      "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "I0320 15:46:18.136960 13716 eval.py:165] \n",
      " _________ text_classification _________\n",
      "I0320 15:46:18.324585 13716 eval.py:185] loss: 0.4704700850144383\n",
      "I0320 15:46:18.328573 13716 eval.py:185] task_name: text_classification\n",
      "I0320 15:46:18.535079 13716 eval.py:185] f1_macro: 0.7494596709749697\n",
      "I0320 15:46:18.538023 13716 eval.py:182] report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       OTHER     0.7989    0.9172    0.8539      2330\n",
      "     OFFENSE     0.7748    0.5524    0.6450      1202\n",
      "\n",
      "    accuracy                         0.7930      3532\n",
      "   macro avg     0.7868    0.7348    0.7495      3532\n",
      "weighted avg     0.7907    0.7930    0.7828      3532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 16:22:02.685729 13716 utils.py:81] device: cpu n_gpu: 0, distributed training: False, automatic mixed precision training: None\n",
      "I0320 16:22:02.705676 13716 infer.py:262] Got ya 1 parallel workers to do inference on 2dicts (chunksize = 128)...\n",
      "I0320 16:22:02.707673 13716 utils.py:244]  0 \n",
      "I0320 16:22:02.708668 13716 utils.py:244] /w\\\n",
      "I0320 16:22:02.709667 13716 utils.py:244] /'\\\n",
      "I0320 16:22:02.710663 13716 utils.py:244] \n",
      "  0%|                                                                                        | 0/2 [00:00<?, ? Dicts/s]\n",
      "Inferencing:   0%|                                                                               | 0/1 [00:00<?, ?it/s]\n",
      "128 Dicts [00:06, 20.16 Dicts/s]                                                                                       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': [{'context': 'Martin ist ein Idiot',\n",
      "                   'end': None,\n",
      "                   'label': 'OFFENSE',\n",
      "                   'probability': 0.85267246,\n",
      "                   'start': None},\n",
      "                  {'context': 'Martin Müller spielt Handball in Berlin',\n",
      "                   'end': None,\n",
      "                   'label': 'OTHER',\n",
      "                   'probability': 0.88546455,\n",
      "                   'start': None}],\n",
      "  'task': 'text_classification'}]\n"
     ]
    }
   ],
   "source": [
    "# Test your model on a sample (Inference)\n",
    "from farm.infer import Inferencer\n",
    "from pprint import PrettyPrinter\n",
    "\n",
    "infer_model = Inferencer(processor=processor, model=model, gpu=True)\n",
    "\n",
    "basic_texts = [\n",
    "    {\"text\": \"Martin ist ein Idiot\"},\n",
    "    {\"text\": \"Martin Müller spielt Handball in Berlin\"},\n",
    "]\n",
    "result = infer_model.inference_from_dicts(dicts=basic_texts)\n",
    "PrettyPrinter().pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
